{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Navigate with Gaussian Processes\n",
    "\n",
    "Imagine you've landed a robot on a remote planet and are now trying to explore that planet to find potential life forms. The spaceship which landed your robot is still orbiting the planet and taking measurements of the elevation. Our goal will be to plan an energy-efficient path on the surface based on those measurements.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src='img/learning_to_plan_illustration.jpg' width=\"800\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "ProbNum Spring School 2023\n",
    "&copy; Jonathan Wenger, 2023 CC BY-NC-SA 3.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Task:**  If you haven't already, install the required packages in a fresh Python environment via the following command:\n",
    "\n",
    "```bash\n",
    "cd 2023_tutorials\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If you can execute the next cell with import statements successfully, everything should work as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_point_clicker import clicker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib widget\n",
    "\n",
    "import tueplots\n",
    "from tueplots import bundles\n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "\n",
    "cmap_norm = mpl.colors.TwoSlopeNorm(vmin=-1.25, vcenter=0.0, vmax=1.25)\n",
    "\n",
    "# Types\n",
    "from typing import Optional\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import scipy\n",
    "import probnum\n",
    "import itergp\n",
    "import pykeops\n",
    "pykeops.set_verbose(False)\n",
    "\n",
    "from probnum import backend\n",
    "from probnum.backend.random import RNGState\n",
    "from probnum.linalg.solvers import ProbabilisticLinearSolver, information_ops, policies, LinearSolverState, beliefs\n",
    "from probnum.problems import LinearSystem\n",
    "from itergp.methods import belief_updates, stopping_criteria\n",
    "\n",
    "# Tutorial-specific code\n",
    "from path_planning.landscapes import generate_landscape\n",
    "from path_planning.dataset import Dataset\n",
    "from path_planning.cost_functions import energy_cost\n",
    "from path_planning.plotting import plot_path_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "Before we blast off into space, we'll start with our robot still on the ground. We'll first explore a simulated landscape to get an idea of the kind of altitude data we'll obtain based off of which we'll decide how to navigate.\n",
    "\n",
    "**Task:** Simulate a few landscapes and corresponding datasets and get a feel for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated landscape\n",
    "seed = 999\n",
    "simulated_landscape_fn = generate_landscape(seed=seed)\n",
    "\n",
    "# Satellite measurements\n",
    "num_train_data = 50000\n",
    "simulated_train_data = Dataset(simulated_landscape_fn, num=num_train_data, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    fig, axs = plt.subplots(ncols=2, sharex=\"row\", sharey=\"row\")\n",
    "\n",
    "    # Plot simulated landscape\n",
    "    x0s = np.linspace(0, 1.0, 100)\n",
    "    x1s = np.linspace(0, 1.0, 100)\n",
    "    X0, X1 = np.meshgrid(x0s, x1s)\n",
    "    X_test = np.vstack([X0.ravel(), X1.ravel()]).T\n",
    "    F = simulated_landscape_fn(X_test).reshape(X0.shape)\n",
    "    \n",
    "    axs[0].contour(X0, X1, F, levels=50, cmap=\"terrain\", norm=cmap_norm)\n",
    "    im = axs[0].imshow(\n",
    "        F, origin=\"lower\", extent=(0.0, 1.0, 0.0, 1.0), cmap=\"terrain\", norm=cmap_norm\n",
    "    )\n",
    "\n",
    "    # Simulated landscape data\n",
    "    n_subset = np.minimum(10000, num_train_data)  # Subset training data for plot\n",
    "    axs[1].scatter(\n",
    "        simulated_train_data.X[0:n_subset, 0],\n",
    "        simulated_train_data.X[0:n_subset, 1],\n",
    "        c=simulated_train_data.y[0:n_subset],\n",
    "        cmap=\"terrain\",\n",
    "        s=0.1,\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # Color bar\n",
    "    fig.colorbar(im, ax=axs.ravel().tolist(), shrink=0.5)\n",
    "\n",
    "    # Plot settings\n",
    "    axs[0].set_aspect(\"equal\")\n",
    "    axs[1].set_aspect(\"equal\")\n",
    "    axs[1].set_xlim(0, 1.0)\n",
    "    axs[1].set_ylim(0, 1.0)\n",
    "    axs[0].set(title=\"Unknown landscape\")\n",
    "    axs[1].set(title=\"Available data\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Planning\n",
    "\n",
    "Now to explore the remote planet we want to move our robot from a starting point to a certain location. However, we only have limited energy resources available and need to find a path that is efficient to drive along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startpoint = np.asarray([0.1, 0.1])\n",
    "endpoint = np.asarray([0.9, 0.9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Based on the simulated landscape data, plan a few paths in the widget below and measure their energy cost. Try to find a path in the simulated landscape that is energy-efficient. What factors likely determine the energy cost?\n",
    "\n",
    "*Hint: You can plan a path by clicking on either \"Path 1\" or \"Path 2\" in the legend. Then add points by left-clicking and remove points by right-clicking near the point to be removed. The starting point is automatically part of the path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Landscape data\n",
    "    ax.scatter(\n",
    "        simulated_train_data.X[0:n_subset, 0],\n",
    "        simulated_train_data.X[0:n_subset, 1],\n",
    "        c=simulated_train_data.y[0:n_subset],\n",
    "        cmap=\"terrain\",\n",
    "        s=0.1,\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # # Simulated landscape\n",
    "    # ax.contour(X0, X1, F, levels=50, cmap=\"terrain\", norm=cmap_norm)\n",
    "    # ax.imshow(\n",
    "    #     F, origin=\"lower\", extent=(0.0, 1.0, 0.0, 1.0), cmap=\"terrain\", norm=cmap_norm\n",
    "    # )\n",
    "\n",
    "    # Plot start and endpoints\n",
    "    ax.scatter(startpoint[0], startpoint[1], marker=\"o\", color=\"C3\", zorder=10)\n",
    "    ax.scatter(endpoint[0], endpoint[1], marker=\"*\", color=\"C3\", zorder=10)\n",
    "\n",
    "    # Manual path planning\n",
    "    path_list = [\"Path 1\", \"Path 2\"]\n",
    "    klicker = clicker(\n",
    "        ax, path_list, markers=[\".\", \".\"], colors=[\"C3\", \"C1\"], linestyle=\"-\"\n",
    "    )\n",
    "    klicker.set_positions({path: np.reshape(startpoint, (-1, 2)) for path in path_list})\n",
    "\n",
    "    # Plot settings\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ax.set_facecolor(\"black\")\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths from widget\n",
    "path1 = np.unique(klicker.get_positions()[\"Path 1\"], axis=0)\n",
    "path2 = np.unique(klicker.get_positions()[\"Path 2\"], axis=0)\n",
    "\n",
    "print(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure energy cost of moving on simulated landscape\n",
    "energy_cost1 = energy_cost(path1, landscape=simulated_landscape_fn)\n",
    "energy_cost2 = energy_cost(path2, landscape=simulated_landscape_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot energy cost along path\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    plot_path_cost(energy_cost1, energy_cost2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we of course do *not* know the actual landscape and therefore cannot measure the energy cost of a planned path until we have actually driven to the target location. Therefore, we have *only one shot* at planning an efficient path based on the measurement data from our satellite. \n",
    "\n",
    "**Questions:** What potential issues make the planning problem hard if we only have measurements from our satellite available? How can we use the altitude data to still plan an efficient path?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Landscape\n",
    "\n",
    "We'll try to learn the surface of the remote planet from the data and then plan a path based on the inferred landscape. Since we want to use energy conservatively, we need a model which can quantify uncertainty arising from missing data to avoid unknown hills or valleys and is robust to noise in the data to avoid taking costly detours. We'll use a Gaussian process for this purpose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's now use the actual data we obtain from the satellite orbiting the remote planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True landscape unknown to our robot\n",
    "landscape_fn = generate_landscape(seed=17)\n",
    "\n",
    "# Satellite measurements\n",
    "num_train_data = 50000\n",
    "train_data = Dataset(landscape_fn, num=num_train_data)\n",
    "train_data.X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "We will model the landscape of the remote planet using a Gaussian process\n",
    "$$f \\sim \\mathcal{GP}(\\mu, k)$$\n",
    "where $\\mu:\\mathbb{R}^2 \\to \\mathbb{R}$ is the prior mean function and $k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ the prior covariance function also known as a kernel.\n",
    "\n",
    "### Prior Information\n",
    "\n",
    "From simulation and previous missions, we have a rough idea of what geological structures on the remote planet look like and how the landscape varies. We can encode this knowledge in the prior by selecting a specific kernel. The lengthscale parameter of the kernel controls how quickly we expect the surface to change. A shorter lengthscale produces a more rapidly varying landscape than a longer lengthscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probnum import randvars, linops\n",
    "from probnum.randprocs import mean_fns\n",
    "from itergp.kernels import Matern\n",
    "from itergp import GaussianProcess, methods\n",
    "\n",
    "input_shape = (2,)\n",
    "\n",
    "# GP prior\n",
    "mean_fn = mean_fns.Zero(input_shape)\n",
    "kernel = Matern(input_shape, lengthscale=0.1, nu=3.5)\n",
    "\n",
    "gp = GaussianProcess(mean_fn, kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Visualize the kernel matrix. What structure can you observe? How do you explain this structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data on a 2D grid\n",
    "x0s = np.linspace(0, 1.0, 50)\n",
    "x1s = np.linspace(0, 1.0, 50)\n",
    "X0, X1 = np.meshgrid(x0s, x1s)\n",
    "X_test = np.vstack([X0.ravel(), X1.ravel()]).T\n",
    "\n",
    "# Plot the kernel matrix\n",
    "plt.close()\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(kernel.matrix(X_test))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** To get a feel for the assumptions encoded in our prior let's visualize it and some corresponding (slices of) draws. Vary the lengthscale parameter to see its effect on the resulting sampled landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw samples from the Gaussian process prior\n",
    "rng_state = backend.random.rng_state(0)\n",
    "samples_2D = gp.sample(rng_state, X_test, sample_shape=(5,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior visualization\n",
    "plt.close()\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0, ncols=2)):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, sharex=True)\n",
    "\n",
    "    # 1D slice with samples\n",
    "    X_test_slice = np.vstack([x0s, 0.5 * np.ones_like(x0s)]).T\n",
    "    gp_mean = gp.mean(X_test_slice)\n",
    "    axs[0].plot(X_test_slice, gp_mean, color=\"C0\")\n",
    "    axs[0].fill_between(\n",
    "        x0s,\n",
    "        y1=gp_mean + 2 * gp.std(X_test_slice),\n",
    "        y2=gp_mean - 2 * gp.std(X_test_slice),\n",
    "        color=\"C0\",\n",
    "        alpha=0.4,\n",
    "    )\n",
    "\n",
    "    axs[0].set(title=\"1D Slice of Prior\")\n",
    "    axs[0].plot(\n",
    "        x0s,\n",
    "        samples_2D.reshape(-1, *X0.shape)[:, :, 0].T,\n",
    "        color=\"C0\",\n",
    "        linestyle=\"--\",\n",
    "        lw=1.0,\n",
    "    )\n",
    "\n",
    "    # Sample of prior\n",
    "    axs[1].imshow(\n",
    "        samples_2D.reshape(-1, *X0.shape)[0, :, :],\n",
    "        origin=\"lower\",\n",
    "        extent=(0.0, 1.0, 0.0, 1.0),\n",
    "        cmap=\"terrain\",\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "    axs[1].set(title=\"Sample from Prior\")\n",
    "\n",
    "    # Plot settings\n",
    "    axs[1].set_aspect(\"equal\")\n",
    "    axs[1].set_xlim(0, 1.0)\n",
    "    axs[1].set_ylim(0, 1.0)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "Let's assume we also have a rough idea of what the noise in the measurement data is that our satellite collects. The scalar $\\sigma^2$ defines the expected squared deviation of a given datapoint from its true altitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood\n",
    "sigma_sq = 0.1\n",
    "noise = randvars.Normal(\n",
    "    mean=np.zeros_like(train_data.y),\n",
    "    cov=linops.Scaling(sigma_sq, shape=(num_train_data, num_train_data)),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "To be able to predict the landscape of the remote planet and to ultimately plan an energy-efficient path, we need to update our prior with the information contained in the satellite measurements. The Gaussian process posterior $f \\sim \\mathcal{GP}(\\mu_{*}, k_{*})$ is fully defined by its mean and covariance function given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mu_*(x) &= \\mu(x) + k(x, X)\\hat{K}^{-1}(y - \\mu(X))\\\\\n",
    "    k_*(x, x') &= k(x, x') - k(x, X)\\hat{K}^{-1}k(X, x')\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Numerical Implementation**:\n",
    "\n",
    "The textbook way to implement the mathematical operations necessary to evaluate the Gaussian process posterior is to solve the linear systems involving the symmetric positive definite kernel Gram matrix $\\hat{K} = k(X, X) + \\sigma^2I$ via a Cholesky factorization. This means we compute a matrix decomposition $$\\operatorname{chol}(K + \\sigma^2I) = LL^\\top,$$ where $L$ is a lower triangular matrix and then solve $\\hat{K}^{-1} b = L^{-\\top}(L^{-1}b)$, where the right hand side is either $b=y - \\mu(X)$ or $b=k(X, x')$. The linear solves with $L$ are efficient since $L$ is triangular.\n",
    "\n",
    "**Task:** Implement the Gaussian process posterior using a Cholesky decomposition as [implemented in SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.cho_solve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "# Implement Gaussian process via Cholesky decomposition\n",
    "\n",
    "def condition_on_data(gp_prior, X_test, X_train, y_train):\n",
    "    # Kernel Gram matrix\n",
    "    gram_matrix = gp_prior.cov.matrix(X_train) + sigma_sq * np.eye(X_train.shape[0])\n",
    "\n",
    "    # Cholesky factorization\n",
    "    # TODO\n",
    "\n",
    "    # Kernel evaluated at training data\n",
    "    KXx = gp_prior.cov.matrix(X_train, X_test)\n",
    "\n",
    "    # Posterior mean\n",
    "    post_mean = gp_prior.mean(X_test) + # TODO\n",
    "\n",
    "    # Posterior covariance\n",
    "    post_cov = (\n",
    "        gp_prior.cov.matrix(X_test)\n",
    "        # TODO\n",
    "    )\n",
    "\n",
    "    return post_mean, post_cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Test your implementation on a small training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = train_data.X[0:100]\n",
    "y_train = train_data.y[0:100]\n",
    "\n",
    "# Test data\n",
    "x0s = np.linspace(0, 1.0, 50)\n",
    "x1s = np.linspace(0, 1.0, 50)\n",
    "X0, X1 = np.meshgrid(x0s, x1s)\n",
    "X_test = np.vstack([X0.ravel(), X1.ravel()]).T\n",
    "\n",
    "# Compute GP posterior\n",
    "post_mean, post_cov = condition_on_data(gp, X_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    fig, axs = plt.subplots(ncols=2, sharex=\"row\", sharey=\"row\")\n",
    "\n",
    "    # Plot inferred landscape\n",
    "    use_uncertainty_overlay = True\n",
    "\n",
    "    if use_uncertainty_overlay:\n",
    "        uncertainty = np.sqrt(np.diag(post_cov))\n",
    "        black_background = np.full((*X0.shape, 3), 0, dtype=np.uint8)\n",
    "        uncertainty_overlay = np.clip(1.0 - uncertainty.reshape(X0.shape), 0.0, 1.0)\n",
    "        axs[0].imshow(black_background)\n",
    "\n",
    "    axs[0].contour(\n",
    "        X0,\n",
    "        X1,\n",
    "        post_mean.reshape(X0.shape),\n",
    "        levels=50,\n",
    "        cmap=\"terrain\",\n",
    "        alpha=0.1,\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "    axs[0].imshow(\n",
    "        post_mean.reshape(X0.shape),\n",
    "        alpha=uncertainty_overlay if use_uncertainty_overlay else None,\n",
    "        origin=\"lower\",\n",
    "        extent=(0.0, 1.0, 0.0, 1.0),\n",
    "        cmap=\"terrain\",\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # Plot start and endpoints\n",
    "    for ax in axs:\n",
    "        ax.scatter(startpoint[0], startpoint[1], marker=\"o\", color=\"C3\", zorder=10)\n",
    "        ax.scatter(endpoint[0], endpoint[1], marker=\"*\", color=\"C3\", zorder=10)\n",
    "\n",
    "    # Data\n",
    "    axs[1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"terrain\", s=0.1)\n",
    "\n",
    "    # Plot settings\n",
    "    axs[0].set_aspect(\"equal\")\n",
    "    axs[1].set_aspect(\"equal\")\n",
    "    axs[1].set_xlim(0, 1.0)\n",
    "    axs[1].set_ylim(0, 1.0)\n",
    "    axs[0].set(title=\"Learned landscape\")\n",
    "    axs[1].set(title=\"Data\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling to Large Datasets\n",
    "\n",
    "Using only a hundred datapoints of the dataset we have available potentially discards important information. We have a lot more data available. Can we use more of the training data to plan a better path?\n",
    "\n",
    "**Task:** Try to condition your implementation of a GP on 50k datapoints. What happens and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large training dataset\n",
    "X_train = train_data.X\n",
    "y_train = train_data.y\n",
    "\n",
    "post_mean, post_var = condition_on_data(gp, X_test, X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to form the kernel matrix $K=k(X, X) \\in \\mathbb{R}^{n \\times n}$ in memory to perform the Cholesky decomposition with 50k datapoints quickly becomes prohibitive both in terms of time and space. Does that mean we cannot use Gaussian processes in the large-scale setting? Not at all as we will see next!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation-aware Gaussian Processes\n",
    "\n",
    "To scale our Gaussian process to larger datasets we could try to approximate it using any number of techniques. However, as we've seen in Prof. Cunningham's talk, this can introduce lots of hidden pathologies in the uncertainty quantification. But this is the main reason, we use a GP in the first place! We want to know which regions to avoid since they may be way to costly to drive through and we might get stuck. \n",
    "\n",
    "Instead, what we need is a way to compute a belief over the landscape with whatever limited resources we have available on our satellite and robot, *without* sacrificing uncertainty quantification. As we will see, we can accept more uncertainty to obtain a faster prediction of the landscape. \n",
    "\n",
    "As a first step, we will tackle the memory issue for large-scale problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-free Kernel Matrices\n",
    "\n",
    "While kernel matrices are generally dense and therefore need $\\mathcal{O}(n^2)$ memory, they are highly structured as we saw earlier. This structure is generally not sparsity, which directly allows memory-efficient storage, but a symbolic representation.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src='img/keops.png' width=\"800\">\n",
    "</p>\n",
    "\n",
    "**Question:** Can you think of a way we could exploit this structure induced by our probabilistic model for efficient computation? Is there a way to multiply with the kernel matrix without having to use $\\mathcal{O}(n^2)$ memory?\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "**Task:** Create a kernel \"linear operator\" via `kernel.linop` instead of `kernel.matrix` which gives a matrix-free representation of the kernel matrix and multiply with a vector with 50k entries. How long does formation of the matrix-free representation take and how long does the matrix-vector product take on your machine?\n",
    "\n",
    "*Hint: The syntax of matrix multiplication is the same for linear operators as for numpy arrays.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-free representation of kernel matrix as \"linear operator\"\n",
    "K = kernel.linop(train_data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large-scale matrix-vector product\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Computation-aware Inference\n",
    "\n",
    "We can make use of a much larger number of measurements to infer the landscape of the remote planet, if we approximate the Gaussian process posterior using matrix-vector multiplication. Recall the form of the Gaussian process posterior $f \\sim \\mathcal{GP}(\\mu_{*}, k_{*})$ from above:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mu_*(x) &= \\mu(x) + k(x, X)\\hat{K}^{-1}(y - \\mu(X))\\\\\n",
    "    k_*(x, x') &= k(x, x') - k(x, X)\\hat{K}^{-1}k(X, x')\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Instead of using a Cholesky decomposition, which requires the dense kernel matrix in memory, we can use an *iterative* probabilistic linear solver to approximate the representer weights $v_* = \\hat{K}^{-1}(y - \\mu(X))$ and the precision matrix $\\hat{K}^{-1}$. In each iteration the solver performs a matrix vector multiplication $v \\mapsto \\hat{K}v$ with the kernel matrix as its main operation, meaning we can use the matrix-free representation from above and never actually have to form $\\hat{K} \\in \\mathbb{R}^{n \\times n}$ in memory. \n",
    "\n",
    "After $i$ iterations the solver returns a combined GP posterior given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mu_i(x) &= \\mu(x) + k(x, X)v_i\\\\\n",
    "    k_i(x, x') &= k(x, x') - k(x, X)C_ik(X, x')\n",
    "\\end{align*}\n",
    "$$\n",
    "where $v_i = C_i (y - \\mu(X)) \\approx v_* = \\hat{K}^{-1}(y - \\mu(X))$ and $C_i \\approx \\hat{K}^{-1}$ is a rank $i$ matrix. Notice the similarities to the mathematical GP posterior above.\n",
    "\n",
    "Now the reason for using a *probabilistic* linear solver is that it quantifies the approximation error from running only a few iterations due to our limited budget. If we perform $i<n$ iterations, the combined posterior is always more uncertain than the mathematical posterior, since we have not invested enough computation yet. This allows us to trade off faster computation for increased uncertainty, in the same way using less data does."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Compute an (approximate) GP posterior which quantifies uncertainty from limited data *and* limited computation via one instance of a probabilistic linear solver below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One variant of a probabilistic linear solver\n",
    "pls = methods.Cholesky(maxrank=100)\n",
    "\n",
    "# Iteratively computed combined GP posterior\n",
    "gp_post = gp.condition_on_data(\n",
    "    train_data.X, train_data.y, noise, approx_method=pls\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One immediately noticable benefit of making this tradeoff is that we can operate on the entire dataset to compute an (approximate) posterior. We can also predict on a much larger number of datapoints.\n",
    "\n",
    "**Task:** Predict the landscape at a million test datapoints using the GP posterior above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal GP prediction on a million datapoints\n",
    "x0s = np.linspace(0, 1.0, 1000)\n",
    "x1s = np.linspace(0, 1.0, 1000)\n",
    "X0, X1 = np.meshgrid(x0s, x1s)\n",
    "X_test = np.vstack([X0.ravel(), X1.ravel()]).T\n",
    "y_test = gp_post.mean(X_test)\n",
    "uncertainty = gp_post.std(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    fig, axs = plt.subplots(ncols=2, sharex=\"row\", sharey=\"row\")\n",
    "\n",
    "    # Plot inferred landscape\n",
    "    use_uncertainty_overlay = True\n",
    "\n",
    "    if use_uncertainty_overlay:\n",
    "        black_background = np.full((*X0.shape, 3), 0, dtype=np.uint8)\n",
    "        uncertainty_overlay = np.clip(1.0 - uncertainty.reshape(X0.shape), 0.0, 1.0)\n",
    "        axs[0].imshow(black_background)\n",
    "\n",
    "    axs[0].contour(\n",
    "        X0,\n",
    "        X1,\n",
    "        y_test.reshape(X0.shape),\n",
    "        levels=50,\n",
    "        cmap=\"terrain\",\n",
    "        alpha=0.1,\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "    axs[0].imshow(\n",
    "        y_test.reshape(X0.shape),\n",
    "        alpha=uncertainty_overlay if use_uncertainty_overlay else None,\n",
    "        origin=\"lower\",\n",
    "        extent=(0.0, 1.0, 0.0, 1.0),\n",
    "        cmap=\"terrain\",\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # Plot start and endpoints\n",
    "    for ax in axs:\n",
    "        ax.scatter(startpoint[0], startpoint[1], marker=\"o\", color=\"C3\", zorder=10)\n",
    "        ax.scatter(endpoint[0], endpoint[1], marker=\"*\", color=\"C3\", zorder=10)\n",
    "\n",
    "    # Data\n",
    "    n_subset = np.minimum(10000, num_train_data)  # Subset training data for plot\n",
    "    axs[1].scatter(\n",
    "        train_data.X[0:n_subset, 0],\n",
    "        train_data.X[0:n_subset, 1],\n",
    "        c=train_data.y[0:n_subset],\n",
    "        cmap=\"terrain\",\n",
    "        s=0.1,\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # Plot settings\n",
    "    axs[0].set_aspect(\"equal\")\n",
    "    axs[1].set_aspect(\"equal\")\n",
    "    axs[1].set_xlim(0, 1.0)\n",
    "    axs[1].set_ylim(0, 1.0)\n",
    "    axs[0].set(title=\"Learned landscape\")\n",
    "    axs[1].set(title=\"Data\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Your Own Policy\n",
    "\n",
    "In each iteration the probabilistic linear solver chooses an *action* $s_i \\in \\mathbb{R}^n$ where the absolute value of each entry roughly describes how much a datapoint should enter into our prediction. The more we target a datapoint, the more certain we become in that region of the input space. This allows us to trade off expensive precision for faster inference with additional uncertainty in regions of the input space where we do not need to know precisely what the landscape is for planning. Next, you will design your own probabilistic linear solver for this specific problem by designing a policy, i.e. a function which for each call returns a new vector $s_i$, that describes where to target computation next.\n",
    "\n",
    "As an example, the simplest policy chooses unit vectors $s_i = (0, \\dots, 0, 1, 0, \\dots, 0)^\\top \\in \\mathbb{R}^n$ in each iteration with a $1$ in the $i$ th entry, signifying we target only the $i$ th datapoint in that iteration. This turns out to be equivalent to sequentially conditioning a GP on the first $i$ datapoints of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitVectorPolicy(policies.LinearSolverPolicy):\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        # Size of kernel matrix\n",
    "        n = solver_state.problem.A.shape[1] \n",
    "        \n",
    "        # Action full of zeros\n",
    "        action = np.zeros(n)\n",
    "\n",
    "        # Set ith entry to 1.0\n",
    "        action[solver_state.step] = 1.0\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Generate a few actions by instantiating a `UnitVectorPolicy` and calling the resulting object multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear system to be solved\n",
    "Khat = kernel.linop(train_data.X) + sigma_sq * linops.Identity(num_train_data)\n",
    "linear_system = LinearSystem(Khat, train_data.y)\n",
    "\n",
    "# Initialize the state of the probabilistic linear solver\n",
    "linear_system_belief = beliefs.LinearSystemBelief(Ainv=linops.Zero(shape=(num_train_data, num_train_data)), b=train_data.y)\n",
    "solver_state = LinearSolverState(linear_system, linear_system_belief)\n",
    "\n",
    "# Define a unit vector policy\n",
    "# TODO\n",
    "\n",
    "# Generated actions\n",
    "for i in range(5):\n",
    "    # TODO: Print result of calling the policy in each step\n",
    "\n",
    "    # Advance the solver to the next step\n",
    "    solver_state.next_step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Another option is to weigh your datapoints according to how large the difference between your model's prediction is and the observed datapoint $y_i$. This corresponds to choosing the current residual $s_i = (y_i - \\mu(x_i)) - \\hat{K}v_i  = y_i - (\\mu(x_i) + \\hat{K}v_i)$ as an action in each iteration. The approximate representer weights in this case are equal to the ones computed by the method of conjugate gradients, a popular linear solver for positive definite linear systems. This is also how libraries like [GPyTorch](https://gpytorch.ai/) work internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualPolicy(policies.LinearSolverPolicy):\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        return solver_state.residual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50384647, -0.8628427 , -0.21751727, ..., -0.41688968,\n",
       "       -0.17926131, -0.52268188])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ResidualPolicy()(solver_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Your next goal will be to design your own policy to predict the landscape of the planet we are trying to drive our robot on. We gave you some policies to start with but you can probably come up with better ones. Think about what datapoints are helpful to target and where you can save compute. How you target the datapoints (i.e. what values the entries of the action have) is also important. Be mindful that actions with few zeros are more costly per iteration than very sparse actions. You can view the predicted landscape and plan a path based on it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomUnitVectorPolicy(policies.LinearSolverPolicy):\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        \n",
    "        # Action full of zeros\n",
    "        action = np.zeros(solver_state.problem.A.shape[1])\n",
    "        \n",
    "        if solver_state.step == 0:\n",
    "            # Random number generator from state.\n",
    "            rng = np.random.default_rng(rng)\n",
    "\n",
    "            # Random permutation of available data indices\n",
    "            shuffled_indices = rng.permutation(solver_state.problem.A.shape[1])\n",
    "\n",
    "            # Store shuffled indices between calls of policy\n",
    "            solver_state.cache[\"shuffled_indices\"] = shuffled_indices\n",
    "\n",
    "        nonzero_idx = solver_state.cache[\"shuffled_indices\"][solver_state.step]\n",
    "        action[nonzero_idx] = 1.0\n",
    "\n",
    "        return action\n",
    "\n",
    "class SubsetPolicy(policies.LinearSolverPolicy):\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        # CG on subset of data\n",
    "        action = np.zeros(solver_state.problem.A.shape[1])\n",
    "        action[0:2000] = solver_state.residual[0:2000]\n",
    "\n",
    "        return action\n",
    "    \n",
    "class SelectionPolicy(policies.LinearSolverPolicy):\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        action = np.zeros(solver_state.problem.A.shape[1])\n",
    "\n",
    "        # Select data on diagonal of domain\n",
    "        idcs = np.where(np.abs(train_data.X[:, 0] - train_data.X[:, 1]) < 0.1)[0]\n",
    "\n",
    "        # Use residual (gradient) actions\n",
    "        action[idcs] = solver_state.residual[idcs]\n",
    "\n",
    "        return action\n",
    "    \n",
    "class LevelSetPolicy(policies.LinearSolverPolicy):\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        action = np.zeros(solver_state.problem.A.shape[1])\n",
    "\n",
    "        # Target data with altitude similar to starting altitude to find level set\n",
    "        idcs = np.where(np.abs(train_data.y - landscape_fn(startpoint)) < 0.05)[0]\n",
    "\n",
    "        # Use residual (gradient) actions\n",
    "        action[idcs] = solver_state.residual[idcs]\n",
    "\n",
    "        return action\n",
    "    \n",
    "class RandomGaussianPolicy(policies.LinearSolverPolicy):\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        # Random number generator from state.\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "        # Random Gaussian actions\n",
    "        return rng.standard_normal(size=(solver_state.problem.A.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPolicy(policies.LinearSolverPolicy):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        \"\"\"Return an action for a given solver state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        solver_state\n",
    "            Current state of the linear solver.\n",
    "        rng\n",
    "            Random number generator state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action\n",
    "            Next action to take. Must have shape=(n,).\n",
    "        \"\"\"\n",
    "        # TODO: come up with your own policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySolver(ProbabilisticLinearSolver):\n",
    "    def __init__(\n",
    "        self,\n",
    "        atol: float = 1e-5,\n",
    "        rtol: float = 1e-5,\n",
    "        maxiter: int = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy=MyPolicy(), # TODO: Swap out the policy here\n",
    "            information_op=information_ops.ProjectedResidualInformationOp(),\n",
    "            belief_update=belief_updates.ProjectedResidualBeliefUpdate(),\n",
    "            stopping_criterion=stopping_criteria.MaxIterationsStoppingCriterion(\n",
    "                maxiter=maxiter,\n",
    "            )\n",
    "            | probnum.linalg.solvers.stopping_criteria.ResidualNormStoppingCriterion(\n",
    "                atol=atol, rtol=rtol\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute combined posterior (uncertainty from limited data + uncertainty from limited computation)\n",
    "gp_post = gp.condition_on_data(\n",
    "    train_data.X, train_data.y, noise, approx_method=MySolver(maxiter=5) \n",
    "    # You can adjust your compute budget by adjusting the number of iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal GP prediction\n",
    "x0s = np.linspace(0, 1.0, 100)\n",
    "x1s = np.linspace(0, 1.0, 100)\n",
    "X0, X1 = np.meshgrid(x0s, x1s)\n",
    "X_test = np.vstack([X0.ravel(), X1.ravel()]).T\n",
    "y_test = gp_post.mean(X_test)\n",
    "uncertainty = gp_post.std(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Plan a path based on the computation-aware GP prediction. Experiment with different policies and try to find one which let's you efficiently plan a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.5)):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, sharex=True)\n",
    "\n",
    "    # Use uncertainty overlay\n",
    "    use_uncertainty_overlay = True\n",
    "\n",
    "    # Evaluate landscape function for plotting\n",
    "    if use_uncertainty_overlay:\n",
    "        black_background = np.full((*X0.shape, 3), 0, dtype=np.uint8)\n",
    "        axs[0].imshow(black_background)\n",
    "        uncertainty_overlay = np.clip(1.0 - uncertainty.reshape(X0.shape), 0.0, 1.0)\n",
    "\n",
    "    axs[0].contour(\n",
    "        X0,\n",
    "        X1,\n",
    "        y_test.reshape(X0.shape),\n",
    "        levels=50,\n",
    "        cmap=\"terrain\",\n",
    "        alpha=0.1,\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "    axs[0].imshow(\n",
    "        y_test.reshape(X0.shape),\n",
    "        alpha=uncertainty_overlay if use_uncertainty_overlay else None,\n",
    "        origin=\"lower\",\n",
    "        extent=(0.0, 1.0, 0.0, 1.0),\n",
    "        cmap=\"terrain\",\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # Uncertainty\n",
    "    axs[1].contour(X0, X1, y_test.reshape(X0.shape), levels=50, alpha=0.1)\n",
    "    axs[1].imshow(\n",
    "        uncertainty.reshape(X0.shape),\n",
    "        origin=\"lower\",\n",
    "        extent=(0.0, 1.0, 0.0, 1.0),\n",
    "    )\n",
    "\n",
    "    # Plot start and endpoints\n",
    "    for ax in axs:\n",
    "        ax.scatter(startpoint[0], startpoint[1], marker=\"o\", color=\"C3\", zorder=10)\n",
    "        ax.scatter(endpoint[0], endpoint[1], marker=\"*\", color=\"C3\", zorder=10)\n",
    "\n",
    "    # Manual path planning\n",
    "    path_list = [\"Path 1\", \"Path 2\"]\n",
    "    klicker = clicker(\n",
    "        axs[0], path_list, markers=[\".\", \".\"], colors=[\"C3\", \"C1\"], linestyle=\"-\"\n",
    "    )\n",
    "    klicker.set_positions({path: np.reshape(startpoint, (-1, 2)) for path in path_list})\n",
    "\n",
    "    # Plot settings\n",
    "    axs[0].set_title(\n",
    "        \"Inferred Landscape with Uncertainty Overlay\"\n",
    "        if use_uncertainty_overlay\n",
    "        else \"Inferred Landscape\"\n",
    "    )\n",
    "    axs[1].set_title(\"Uncertainty\")\n",
    "    axs[0].set_aspect(\"equal\")\n",
    "    axs[1].set_aspect(\"equal\")\n",
    "    axs[0].set_xlim(0, 1.0)\n",
    "    axs[0].set_ylim(0, 1.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Predict the energy cost of your planned paths using the computation-aware Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths from widget\n",
    "path1 = np.unique(klicker.get_positions()[\"Path 1\"], axis=0)\n",
    "path2 = np.unique(klicker.get_positions()[\"Path 2\"], axis=0)\n",
    "\n",
    "# Predict energy cost of based on GP\n",
    "energy_cost1 = energy_cost(path1, landscape=gp_post.mean)\n",
    "energy_cost2 = energy_cost(path2, landscape=gp_post.mean)\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    plot_path_cost(energy_cost1, energy_cost2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Try to refine your policy above to obtain a more useful prediction for path planning. Think about where you can save computation by accepting more (computational) uncertainty. Can you plan a path that is predicted to be more efficient? How much can you trust that prediction?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ATTENTION: Do not continue below until prompted by the tutorial organizers.** You don't want to spoil the fun, do you? :)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Energy Cost\n",
    "\n",
    "Let's see how good the path is you've chosen based on the learned landscape!\n",
    "\n",
    "**Task:** Drive your robot along the defined paths above and measure the true energy cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, sharex=True)\n",
    "\n",
    "    # Inferred landscape with uncertainty overlay\n",
    "    axs[0].imshow(black_background)\n",
    "    axs[0].contour(\n",
    "        X0,\n",
    "        X1,\n",
    "        y_test.reshape(X0.shape),\n",
    "        levels=50,\n",
    "        cmap=\"terrain\",\n",
    "        alpha=0.1,\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "    axs[0].imshow(\n",
    "        y_test.reshape(X0.shape),\n",
    "        alpha=uncertainty_overlay if use_uncertainty_overlay else None,\n",
    "        origin=\"lower\",\n",
    "        extent=(0.0, 1.0, 0.0, 1.0),\n",
    "        cmap=\"terrain\",\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # Actual landscape\n",
    "    y_true = landscape_fn(X_test)\n",
    "    axs[1].contour(\n",
    "        X0, X1, y_true.reshape(X0.shape), levels=50, alpha=0.1, norm=cmap_norm\n",
    "    )\n",
    "    axs[1].imshow(\n",
    "        y_true.reshape(X0.shape),\n",
    "        origin=\"lower\",\n",
    "        extent=(0.0, 1.0, 0.0, 1.0),\n",
    "        cmap=\"terrain\",\n",
    "        norm=cmap_norm,\n",
    "    )\n",
    "\n",
    "    # Plot start and endpoints\n",
    "    for ax in axs[0:2]:\n",
    "        ax.scatter(startpoint[0], startpoint[1], marker=\"o\", color=\"C3\", zorder=10)\n",
    "        ax.scatter(endpoint[0], endpoint[1], marker=\"*\", color=\"C3\", zorder=10)\n",
    "\n",
    "    # Plot paths\n",
    "    for ax in axs[0:2]:\n",
    "        ax.plot(\n",
    "            path1[:, 0], path1[:, 1], \"-o\", markersize=2, color=\"C3\", label=\"Path 1\"\n",
    "        )\n",
    "        ax.plot(\n",
    "            path2[:, 0], path2[:, 1], \"-o\", markersize=2, color=\"C1\", label=\"Path 2\"\n",
    "        )\n",
    "\n",
    "    # Plot settings\n",
    "    axs[0].set_title(\n",
    "        \"Inferred Landscape with Uncertainty Overlay\"\n",
    "        if use_uncertainty_overlay\n",
    "        else \"Inferred Landscape\"\n",
    "    )\n",
    "    axs[1].set_title(\"True Landscape\")\n",
    "    axs[0].set_aspect(\"equal\")\n",
    "    axs[1].set_aspect(\"equal\")\n",
    "    axs[0].set_xlim(0, 1.0)\n",
    "    axs[0].set_ylim(0, 1.0)\n",
    "\n",
    "    # Legend\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles=handles,\n",
    "        labels=labels,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 0.05),\n",
    "        fancybox=False,\n",
    "        shadow=False,\n",
    "        ncol=2,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Compare with the people sitting next to you who found the path with the least energy cost! No cheating and changing the path after having seen the true landscape allowed ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure energy cost of moving on actual landscape\n",
    "energy_cost1 = energy_cost(path1, landscape=landscape_fn)\n",
    "energy_cost2 = energy_cost(path2, landscape=landscape_fn)\n",
    "\n",
    "with plt.rc_context(bundles.neurips2022(rel_width=1.0)):\n",
    "    plot_path_cost(energy_cost1, energy_cost2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Takeaways\n",
    "- GPs scale to large datasets!\n",
    "- GPs allow us to trade off accuracy for uncertainty, exactly and explicitly.\n",
    "- Limited data *and* limited computation induce uncertainty.\n",
    "\n",
    "### Extensions?\n",
    "\n",
    "How could we improve the path planning even further? Let's hear your suggestions!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Software Libraries\n",
    "If you've enjoyed this tutorial, you can support the developers of the underlying libraries, including us, by starring the underlying libraries on GitHub:\n",
    "\n",
    "- [ProbNum](https://github.com/probabilistic-numerics/probnum): Probabilistic Numerics in Python.\n",
    "- [IterGP](https://github.com/JonathanWenger/itergp): Computation-aware Gaussian Processes.\n",
    "- [KeOps](https://github.com/getkeops/keops): Kernel Operations on the GPU, with autodiff, without memory overflows.\n",
    "- [GPyTorch](https://github.com/cornellius-gp/gpytorch): Gaussian processes with GPU acceleration.\n",
    "\n",
    "### Research\n",
    "If you want to read more about the research this tutorial is based on, check out the following paper:\n",
    "\n",
    "> J. Wenger, G. Pleiss, M. Pfrtner, P. Hennig, and J. P. Cunningham. Posterior and\n",
    "Computational Uncertainty in Gaussian Processes. In: *Advances in Neural Information\n",
    "Processing Systems (NeurIPS)*. 2022. [arXiv: 2205.15449](https://arxiv.org/abs/2205.15449)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probnum-spring-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
